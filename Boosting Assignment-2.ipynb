{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9187a13-3a94-457d-8ec9-751ae7fb7b54",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3e656-48fe-47b7-95d6-40273dd59a22",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, which involve predicting a continuous numerical value as the output. It is a type of ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model. Gradient Boosting Regression is a part of the broader family of gradient boosting algorithms.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. Initialization: Gradient Boosting starts with an initial prediction, which can be a simple estimate like the mean of the target values for regression tasks.\n",
    "\n",
    "2. Fitting Weak Learners: It then fits a weak learner (usually a shallow decision tree) to the training data. The weak learner aims to capture the errors (residuals) made by the current model's predictions on the training data.\n",
    "\n",
    "3. Update Model: The model is updated by adding the predictions from the weak learner to the previous model's predictions. This update is done in a way that minimizes the loss function, which quantifies the difference between the predicted values and the actual target values.\n",
    "\n",
    "4. Iterative Process: Steps 2 and 3 are repeated iteratively. In each iteration, a new weak learner is added, and the model is updated to correct the errors made by the previous model.\n",
    "\n",
    "5. Learning Rate: There's a hyperparameter called the learning rate that controls the contribution of each weak learner to the final model. A smaller learning rate makes the process more robust but requires more iterations, while a larger learning rate can speed up convergence but may lead to overshooting.\n",
    "\n",
    "6. Stopping Criterion: The process continues until a predefined stopping criterion is met, such as reaching a maximum number of iterations or when the model's performance on a validation set no longer improves.\n",
    "\n",
    "7. Final Model: The final prediction is the sum of predictions from all the weak learners, weighted by their respective contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb48f9a-03c9-43d5-a9ef-5ccca1b1c943",
   "metadata": {},
   "source": [
    "Q2.  Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a \n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's \n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4fa6b-98d0-41e5-b77a-7414b8c03ca7",
   "metadata": {},
   "source": [
    "Implementing a simple gradient boosting algorithm from scratch is a non-trivial task, but I can provide you with a simplified version to demonstrate the basic principles. Keep in mind that professional-grade implementations like XGBoost or LightGBM are highly optimized and feature-rich, and creating a complete, efficient implementation from scratch would be quite extensive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85198845-7c68-4cf7-b949-000089c20f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4081\n",
      "R-squared: 0.5977\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Define the number of boosting rounds (weak learners)\n",
    "n_rounds = 100\n",
    "\n",
    "# Initialize the model's predictions with the mean of the target values\n",
    "predictions = np.mean(y) * np.ones_like(y)\n",
    "\n",
    "# Gradient boosting loop\n",
    "for _ in range(n_rounds):\n",
    "    # Calculate the residuals (negative gradient)\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Fit a weak learner (decision stump in this case)\n",
    "    weak_learner = DecisionTreeRegressor(max_depth=1)\n",
    "    weak_learner.fit(X, residuals)\n",
    "    \n",
    "    # Make predictions with the weak learner\n",
    "    weak_predictions = weak_learner.predict(X)\n",
    "    \n",
    "    # Update the model's predictions by adding the weak learner's predictions\n",
    "    predictions += weak_predictions\n",
    "\n",
    "# Calculate Mean Squared Error and R-squared\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750bcdd7-5e6e-402d-846e-fbdaeb60253f",
   "metadata": {},
   "source": [
    "Q3.Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to \n",
    "optimise the performance of the model. Use grid search or random search to find the best \n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655c1c0f-19e6-4308-be62-d2e3626c3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4b1d0-9c41-489b-aa00-7c694f237693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 2, 3],\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(gbm, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model to find the best hyperparameters\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train a model with the best hyperparameters\n",
    "best_gbm = GradientBoostingRegressor(**best_params)\n",
    "best_gbm.fit(X, y)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_gbm.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a62ed-3ff2-4bb9-a27f-f27e0c380694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76843893-1795-4221-a3d4-92ff7a0f487a",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db144b-c3f0-467c-a548-ba6b77e3afb9",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner is a machine learning model that is relatively simple and performs slightly better than random guessing on a given problem. The term \"weak\" does not imply that the model is inherently poor; instead, it suggests that the model's performance is only marginally better than chance.\n",
    "\n",
    "Key characteristics of a weak learner in Gradient Boosting include:\n",
    "\n",
    "1. Low Complexity: Weak learners are typically simple models with low complexity. For example, a decision stump (a decision tree with only one split) is a common choice for a weak learner.\n",
    "\n",
    "2. Low Bias, High Variance: Weak learners have low bias, meaning they can capture some patterns in the data, but they have high variance, which implies that they are sensitive to noise and can easily overfit the training data.\n",
    "\n",
    "3. Limited Predictive Power: Weak learners alone are not capable of making accurate predictions for the problem at hand. Their predictions may be slightly better than random guessing, but they are usually far from being highly accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375e7e4-7c4b-43c8-89ea-0b28d244bfc3",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4e7bb-6bf8-4a40-8c86-fce6c4dc2e36",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the metaphor of a team of experts trying to solve a problem collaboratively. Here's a simplified explanation of the intuition behind Gradient Boosting:\n",
    "\n",
    "1. Team of Experts: Imagine you have a team of experts, each of whom is good at solving a specific aspect of a problem but not the entire problem. Each expert is like a \"weak learner\" in machine learning terms, meaning they can make predictions that are slightly better than random guessing for their specific area of expertise.\n",
    "\n",
    "2. Team's Performance: Initially, the team's overall performance is not very impressive because each expert's predictions are limited. However, the team's strength lies in their ability to learn and adapt from their mistakes.\n",
    "\n",
    "3. Sequential Improvement: The team members work sequentially. The first expert makes predictions, and the team evaluates their performance. Then, the second expert comes in and focuses on correcting the mistakes made by the first expert. This process continues with each expert correcting the combined mistakes of the previous team members.\n",
    "\n",
    "4. Weighted Contributions: Each expert's opinion is valuable, but some experts are better than others. In Gradient Boosting, the contributions of these experts are weighted based on their individual strengths. The algorithm assigns higher weight to experts who are more accurate and reliable.\n",
    "\n",
    "5. Ensemble Prediction: The final prediction is made by combining the predictions of all the experts, taking into account their weighted opinions. This ensemble prediction is often much more accurate than the prediction of any individual expert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d81687-9cf3-490d-8f75-c019b3fec031",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47347a97-71cd-4ccf-a686-d9a19221429d",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially, with each weak learner aiming to correct the errors made by the combined predictions of the previous models. Here's a step-by-step explanation of how Gradient Boosting builds this ensemble:\n",
    "\n",
    "1. Initialization:\n",
    "\n",
    "Initialize the ensemble's predictions to a constant value, often the mean of the target values. This serves as the initial prediction.\n",
    "Calculate the initial residuals (errors) by subtracting the initial predictions from the actual target values.\n",
    "\n",
    "2. Sequential Addition of Weak Learners:\n",
    "\n",
    "For a predefined number of iterations (or until a stopping criterion is met), Gradient Boosting adds weak learners to the ensemble one by one.\n",
    "In each iteration, it fits a new weak learner (often a decision tree with limited depth) to the residuals from the previous step. The weak learner's task is to capture and correct the errors made by the current ensemble of predictions.\n",
    "The weak learner is trained to minimize the loss function (e.g., mean squared error or any other suitable loss function) with respect to the residuals.\n",
    "\n",
    "3. Updating Predictions:\n",
    "\n",
    "Once the weak learner is trained, its predictions are combined with the current ensemble's predictions. However, these new predictions are typically not added directly. Instead, they are scaled by a learning rate (shrinkage parameter) to control the contribution of the weak learner. This prevents the algorithm from overfitting.\n",
    "The scaled predictions are added to the current ensemble's predictions, which updates the ensemble's predictions.\n",
    "\n",
    "4. Updating Residuals:\n",
    "\n",
    "After updating the predictions, calculate the new residuals by subtracting the updated predictions from the actual target values.\n",
    "\n",
    "5. Weighting and Convergence:\n",
    "\n",
    "The process of adding weak learners and updating predictions and residuals continues for the specified number of iterations or until a stopping criterion is met. The ensemble assigns weights to each weak learner based on their performance in reducing the residuals.\n",
    "\n",
    "6. Final Prediction:\n",
    "\n",
    "The final prediction is obtained by summing up the predictions from all the weak learners in the ensemble. Each weak learner's prediction is weighted by its contribution to error reduction during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029d4ed-d23d-4027-9274-a1828132e8d0",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting \n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d5dc3-1e44-44fd-b844-a0464e48583a",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the key mathematical concepts and principles behind the algorithm. Here are the steps involved in developing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. Loss Function and Residuals:\n",
    "\n",
    "Start with the definition of a loss function, which quantifies the difference between the model's predictions and the actual target values. Common loss functions for regression tasks include Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n",
    "Understand that the goal of Gradient Boosting is to minimize this loss function.\n",
    "Define the residuals as the negative gradient (or derivative) of the loss function with respect to the model's predictions. Residuals represent how much the current model's predictions deviate from the true target values.\n",
    "\n",
    "2. Initialization:\n",
    "\n",
    "Initialize the ensemble's predictions as a constant value, often the mean of the target values.\n",
    "Calculate the initial residuals by subtracting the initial predictions from the actual target values.\n",
    "\n",
    "3. Weak Learners:\n",
    "\n",
    "Understand that each weak learner's task is to capture and correct the errors (residuals) made by the current ensemble of predictions.\n",
    "\n",
    "4. Fitting Weak Learners:\n",
    "\n",
    "Explain how, in each iteration, a new weak learner is trained on the dataset. It aims to minimize the loss function with respect to the current residuals.\n",
    "Show how the weak learner adjusts its predictions to reduce the errors, effectively moving the model closer to the true target values.\n",
    "\n",
    "5. Update Predictions and Residuals:\n",
    "\n",
    "Describe how the predictions from the new weak learner are scaled by a learning rate (a small positive value) and added to the current ensemble's predictions. This update step gradually adjusts the ensemble.\n",
    "Calculate the new residuals by subtracting the updated predictions from the actual target values.\n",
    "\n",
    "7. Weighting and Convergence:\n",
    "\n",
    "Explain that the algorithm assigns weights to each weak learner based on their contributions to reducing the residuals. Weak learners that perform well receive higher weights.\n",
    "Clarify that the process of adding weak learners and updating predictions and residuals continues for a specified number of iterations or until a stopping criterion is met.\n",
    "\n",
    "8. Final Prediction:\n",
    "\n",
    "Emphasize that the final prediction is obtained by summing up the predictions from all the weak learners in the ensemble. Each weak learner's prediction is weighted by its contribution to error reduction during training.\n",
    "Highlight that the ensemble's predictions have been refined iteratively to minimize the loss function, leading to a powerful predictive model.\n",
    "\n",
    "9. Regularization and Hyperparameters:\n",
    "\n",
    "Introduce the concept of regularization techniques, such as controlling the learning rate and limiting the depth of weak learners' trees, to prevent \n",
    "\n",
    "10. overfitting.\n",
    "\n",
    "Mention that hyperparameter tuning, such as choosing the number of iterations, learning rate, and tree depth, plays a crucial role in optimizing the algorithm's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe352149-90d3-4e7c-8f78-d8a645841a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53996227-e090-4312-8dbe-2528641d7f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b85c37-d6df-4add-b4a3-8ae11a6bac58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee43bbb-3591-487b-a6da-f28f31958c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4af095-eddd-4e7e-ad7e-bee4111d539e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
